# **PythonSpider：解析数据**
### 前言：
我们在获取数据（前一篇文章）中，得到了一页网页的数据，接着在循环中得到所有服务器给出的数据。然而这些所有数据并不全是我们想要的内容，所以需要去解析。

又因为**解析数据是一页一页去解析，因此，循环中每次得到一页的数据后，就要紧接着对其进行数据解析筛选，然后存储到一个列表(任何容器)当中（该列表是我们想要得到的所有已筛选数据）。**

解析数据是指在用函数得到一页网页的数据以后，对当前页面的数据进行筛选解析。
前章代码：
```python
def getData(base_url):  # 获取网页数据  
    data_list = []  
    # 以豆瓣网为例，10页的数据  
    for i in range(0, 10):  
        url = base_url + str(i * 25)  
        response_info = askURL(url)  
        print(response_info)  
    return data_list  
```
我们会在此基础上进行修改。
为进行数据解析，我们还会大致介绍正则表达式。
## 目录
1. [正则表达式（re模块的使用）](#正则表达式（re模块的使用）)
2. [解析数据完整模板](#解析数据)
---
### 1. 正则表达式（re模块的使用）
正则表达式：就是通过一个文本模式来匹配一组符合条件的字符串，即对目标字符串进行过滤操作。通常被用来检索、替换那些符合某个字符串模式的文本。在python中利用re模块，可实现利用正则表达式来达到需求。
```python
# re模块的基础使用
import re  
pat = re.compile("AA")  # 此处的AA是正则表达式，用来去验证其他的字符串   
m = pat.search("ACBAA")  #   
m = pat.search("ACBAACDANCJKAA")  #    
m = re.search("AA", "AANCISLAA")    
m = re.findall("[A-Z]+", "AANCIaSLAA")    
m = re.sub("A", "a", "AANCIaSLAA")  # 替换，将原串中的A替换为a
```
正则表达式语言由两种基本字符类型组成：原义（正常）文本字符和元字符。元字符使正则表达式具有处理能力。所谓元字符就是指那些在正则表达式中具有特殊意义的专用字符，可以用来规定其前导字符（即位于元字符前面的字符）在目标对象中的出现模式。

---
[点击返回目录](#目录)
### 2. 解析数据
我们以豆瓣网top250电影为例子，假设我们的需求是250个电影的：**视频链接、影片片名、影片评分、影片评价人数、影片概况、影片的相关内容等数据**。
```python
# 影片视频的链接
find_link = re.compile(r'<a href="(.*?)">')  
# 影片图片的链接  
find_img = re.compile(r'<img.*src="(.*?)">', re.S)  # 让换行符包含在字符中  
# 影片的片名  
find_title = re.compile(r'<span class="title">(.*)</span')  
# 影片的评分  
find_rating = re.compile(r'<span class="rating_num" property="v:average:>(.*)</span>')  
# 评价人数  
find_judge = re.compile(r'<span>(\d*)人评价</span>')  
# 找到概况  
find_inq = re.compile(r'<span class="inq">(.*)</span>')  
# 找到影片的相关内容  
find_bd = re.compile(r'<p class="">(.*?)</p>', re.S)
```
```python
def getData(base_url):  # 获取网页数据  
    data_list = []  
    # 以豆瓣网为例，10页的数据  
    for i in range(0, 10):  
        url = base_url + str(i * 25)  
        response_info_html = askURL(url)  
        # 逐一解析数据  
        soup = BeautifulSoup(response_info_html, "html.parser")  
        for item in soup.find_all('div', class_="item"):  
            # item 一部电影包含div 和 类名为item 的信息  
            # print(item) 测试：查看电影item全部信息  
            data = []  # 保存一部电影的所有信息  
            item = str(item)  
  
            # 影片详情的链接，以及保存数据  
            link = re.findall(find_link, item)[0]  
            data.append(link)  # 添加链接  
  
            img_src = re.findall(find_img, item)[0]  
            data.append(img_src)  # 添加图片  
  
            title = re.findall(find_title, item)  # 片名可能只有一个  
            if len(title) >= 2:  # 中英文名，即有两个名字  
                ch_title = title[0]  
                data.append(ch_title)  
                eng_title = title[1].replace("/", "")  # 去掉无关符号  
                data.append(eng_title)  # 添加外国名  
            else:  
                data.append(title[0])  
                data.append(' ')  # 外文名留空  
            # 添加评分  
            rating = re.findall(find_rating, item)  
            data.append(rating)  
            # 添加评价人数  
            judge_num = re.findall(find_judge, item)  
            data.append(judge_num)  
            # 添加概述  
            inq = re.findall(find_inq, item)  # 有可能没有概述  
            if len(inq) != 0:  
                inq = inq[0].replace("。", "")  # 去掉句号  
                data.append(inq)  
            else:  
                data.append(" ")  # 留空  
            # 添加bd  
            bd = re.findall(find_bd, item)[0]  
            bd = re.sub('<br(\s+)?/>(\s+)?', " ", bd)  # 去掉<br/>  
            bd = re.sub('/', " ", bd)  # 替换/  
            data.append(bd.strip())  # 去掉前后的空格  
            data_list.append(data)  # 把处理好的一部电影信息放入datalist当中  
    return data_list
```
自行阅读，我要摆烂，不再解释了。总之，不要一味的复制，在这代码里面，是根据原来网页的源码进行编写的（F12），要学会自行变通和理解。
比如：
1. 在开头建立的所有正则表达式对象，都是根据需求按照源码编写的。
2. 添加影片名称的时候，之所以是上述代码这样，是因为原网站中电影的名字可能会有两个，其实甚至可能更多。
3. 同理，概述也是如此。
4. for item in soup.find_all('div', class_="item"):  比如这一句代码，其实也是对照这源码来写的，得到的结果就是标签为div，类名为item的所有子内容；紧接着后面的一系列操作（利用正则表达式来进行筛选匹配），范围也是仅仅在这子内容里面。
**总之，进行数据解析时，需要对照着实际情况来，不能一味的复制。**
[点击返回目录](#目录)
---
