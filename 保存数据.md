# **PythonSpider：保存数据**
### **前言：**
我们已经得到了数据并且存储到了data_list中。
接下来，我们要做的就是存储数据，主要分为两大类：1是利用xlwt库存储到excel表格当中；2是利用SQLite存储到数据库当中；其中还会介绍SQLite和xlwt这两个库以及拓展。
## 目录：
1. [xlwt库](#xlwt库)
2. [保存数据-Excel](#保存数据-excel)
3. [SQLite库](#SQLite库)
4. [保存数据-SQLite](#保存数据-sqlite)
5. [豆瓣网电影top250整体代码模板](#豆瓣网电影top250整体代码模板)
### 1. xlwt库
在python中可以利用xlwt，来创建一个表格的对象，然后在该对象表格上又进行其他操作，比如添加表格Sheet。
```python
#基础操作
work_book = xlwt.Workbook(encoding="utf-8")  # 创建xlwt表格对象，编码与解码方式为utf-8
work_sheet = work_book.add_sheet("sheet1")  # 创建工作表  
work_sheet.write(0, 0, "hello")  # 写入数据，第一行参数是“行”，第二行参数是“列”，第三个参数是写入的内容  
work_book.save("student.xls")  # 保存数据
```
### 2. 保存数据-excel
```python
# 以豆瓣网top250电影为例
def saveDate(data_list, save_path):  # 保存数据  
    work_book = xlwt.Workbook(encoding="utf-8", style_compression=0)  # 创建xlwt表格对象  
    work_sheet = work_book.add_sheet("豆瓣电影top250", cell_overwrite_ok=True)  # 创建工作表，可覆盖重写  
    col = ("电影详情链接", "图片链接", "影片中文名", "影片外国名", "评分", "评价数", "概况", "相关信息")  
    # 列  
    for col_i in range(0, 8):  
        work_sheet.write(0, col_i, col[col_i])  
    for num in range(0, 250):  
        data = data_list[num]  # 第num个电影的信息  
        for col_j in range(0, 8):  
            work_sheet.write(num+1, col_j, data[col_j])  
    work_book.save(save_path)  # 保存数据
```
### 3. SQLite库
1. 链接数据库
```python
import sqlite3
conn = sqlite3.connect("test.db")  # 打开或者创建数据库文件
```
2. 创建数据表
```python
conn = sqlite3.connect("test.db")   # 打开或者创建数据库文件  
c = conn.cursor()  # 获取游标  
sql = '''  
    create table company        (id int primary key not null,        name text not null,        age int not null,        address char(50),        salary real);'''  
c.execute(sql)  # 执行sql语句  
conn.commit()  # 提交数据库操作  
conn.close()  # 关闭数据库连接
```
3. 插入数据
```python
conn = sqlite3.connect("test.db")   # 打开或者创建数据库文件  
c = conn.cursor()  # 获取游标  
sql1 = '''  
    insert into company(id, name, age, address, salary)        values (1, '张三', 32, "成都", 8000);  
'''  
sql2 = '''  
    insert into company(id, name, age, address, salary)        values (2, '李四', 30, "重庆", 15000);  
'''  
  
c.execute(sql1)  # 执行sql语句  
c.execute(sql2)  
conn.commit()  # 提交数据库操作  
conn.close()  # 关闭数据库连接
'''  
  
c.execute(sql1)  # 执行sql语句  
c.execute(sql2)  
conn.commit()  # 提交数据库操作  
conn.close()  # 关闭数据库连接
```
4. 查找数据
```python
conn = sqlite3.connect("test.db")   # 打开或者创建数据库文件  
c = conn.cursor()  # 获取游标  
sql3 = "select id, name, age, address, salary from company"  
cursor = c.execute(sql3)  # 执行sql语句  
for row in cursor:  
    print("id = ", row[0])  
    print("name = ", row[1])  
    print("age = ", row[2])  
    print("address = ", row[3])  
    print("salary = ", row[4], "\n")  
conn.close()  # 关闭数据库连接
```
### 4. 保存数据-sqlite
以豆瓣网电影top250为例子
```python
def saveDateDB(data_list, db_path):  
    # init_db(db_path)  # 已经建表了 
    conn = sqlite3.connect(db_path)  
    cur = conn.cursor()  
    for data in data_list:  
        for index in range(len(data)):  
            if index == 5 or index == 4:  
                continue  
            data[index] = '"' + str(data[index]) + '"'  
        sql = '''  
            insert into movie250(            info_link, pic_link, name, ename, score, rated, introduction, info)             values(%s)  
             ''' % ",".join(data)  # 此处是用%s先占位，然后将data里面8  
        # 列（一部电影）的所有信息分别用逗号隔开；将分割后的字符串放到占位的地方上去，此处简洁操作，值得学习。  
        cur.execute(sql)  
        conn.commit()  
    cur.close()  
    conn.close()  


def init_db(db_path):  
    sql = '''  
        create table movie250        (        id integer primary key autoincrement,        info_link text,        pic_link text,        name varchar,        ename varchar,        score numeric,        rated numeric,        introduction text,        info text        )    '''  # 创建数据表  
    conn = sqlite3.connect(db_path)  
    cursor = conn.cursor()  
    cursor.execute(sql)  
    conn.commit()  
    conn.close()
```
### 5. 豆瓣网电影top250整体代码模板
```python
# -*- coding = utf-8 -*-  
from bs4 import BeautifulSoup  # 网页解析，获取数据  
import re  # 正则表达式，进行文字匹配  
import urllib.request, urllib.error, urllib.parse  # 模拟浏览器。制定URL，获取网页数据  
import xlwt  # 进行excel操作，存储到excel当中  
import sqlite3  # 进行SQLite数据库操作  
  
def main():  
    base_url = "https://movie.douban.com/top250?start="  
    # 1.获取网页数据  
    data_list = getData(base_url)  
    # save_path = ".\\豆瓣电影Top250.xls"  
    # save_path = "student.xls"    db_path = "movie250.db"  
    # 3.保存数据  
    # saveDate(data_list, save_path)  
    saveDateDB(data_list, db_path)  
  
find_link = re.compile(r'<a href="(.*?)">')  
# 影片图片的链接  
find_img = re.compile(r'<img.*src="(.*?)"', re.S)  # 让换行符包含在字符中  
# 影片的片名  
find_title = re.compile(r'<span class="title">(.*)</span>')  
# 影片的评分  
find_rating = re.compile(r'<span class="rating_num" property="v:average:>(.*)</span>')  
# 评价人数  
find_judge = re.compile(r'<span>(\d*)人评价</span>')  
# 找到概况  
find_inq = re.compile(r'<span class="inq">(.*)</span>')  
# 找到影片的相关内容  
find_bd = re.compile(r'<p class="">(.*?)</p>', re.S)  
  
def getData(base_url):  # 获取网页数据  
    data_list = []  
    # 以豆瓣网为例，10页的数据  
    for i in range(0, 10):  
        url = base_url + str(i * 25)  
        response_info_html = askURL(url)  
        # 逐一解析数据  
        soup = BeautifulSoup(response_info_html, "html.parser")  
        for item in soup.find_all('div', class_="item"):  
            # item 一部电影包含div 和 类名为item 的信息  
            # print(item) 测试：查看电影item全部信息  
            data = []  # 保存一部电影的所有信息  
            item = str(item)  
  
            # 影片详情的链接，以及保存数据  
            link = re.findall(find_link, item)[0]  
            data.append(link)  # 添加链接  
  
            img_src = re.findall(find_img, item)[0]  
            data.append(img_src)  # 添加图片  
  
            title = re.findall(find_title, item)  # 片名可能只有一个  
            if len(title) >= 2:  # 中英文名，即有两个名字  
                ch_title = title[0]  
                data.append(ch_title)  
                eng_title = title[1].replace("/", "")  # 去掉无关符号  
                data.append(eng_title)  # 添加外国名  
            else:  
                data.append(title[0])  
                data.append(' ')  # 外文名留空  
            # 添加评分  
            rating = re.findall(find_rating, item)  
            data.append(rating)  
            # 添加评价人数  
            judge_num = re.findall(find_judge, item)  
            data.append(judge_num)  
            # 添加概述  
            inq = re.findall(find_inq, item)  # 有可能没有概述  
            if len(inq) != 0:  
                inq = inq[0].replace("。", "")  # 去掉句号  
                data.append(inq)  
            else:  
                data.append(" ")  # 留空  
            # 添加bd  
            bd = re.findall(find_bd, item)[0]  
            bd = re.sub('<br(\s+)?/>(\s+)?', " ", bd)  # 去掉<br/>  
            bd = re.sub('/', " ", bd)  # 替换/  
            data.append(bd.strip())  # 去掉前后的空格  
            data_list.append(data)  # 把处理好的一部电影信息放入datalist当中  
    return data_list  
def askURL(url):  # 获取一页的网页数据  
    request_headers = {  
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36",  
        "Cookie": "__guid = 234898968.4130034644330370600.1668440054847.2527;_uc_m2 = 5ae3ba3937f601d44fda97e11ddc71946df1adc32b4e;_uc_mid = 44824b875d2d84d618386ee41743b610;__huid = 11Um5aFkUYZ0tiXa4IbdSod9Nig % 2F % 2FqcBQtuIU2oE11Tw4 % 3D;__hsid = 717de45fbbd75401;__DC_monitor_count = 1;count = 1;sessionID = 234898968.2452920065724183600.1669037804860.273;__asc = bd319522680e50bd1a1636fdb161e95a.1086.1669037805496;__asc2 = 1f05a42ca8402376164369f71e444ed22817b3d6b4f8.11c0.1669037805496;test_cookie_enable = null;customEng = 11 - 21;festival_fifaqatar_v1 = 1"  
    }  # 注意拼写一定要正确，比如User-Agent，一定要严格按照格式，比如如果出现空格，User - Agent，则也会被服务器识别出来  
    try:  
        request = urllib.request.Request(url, headers=request_headers)  # 重新封装  
        response_info = urllib.request.urlopen(request)  # 模拟浏览器发送请求  
        return response_info.read().decode('utf-8')  # 返回网页源码，如果需要更多的信息，改变调用的方法即可  
    except urllib.error.URLError as urlError:  # 异常处理，有可能会出现浏览器内部的问题，404等等问题  
        if hasattr(urlError, "code"):  
            print(urlError.code)  
        if hasattr(urlError, "reason"):  
            print(urlError.reason)  
    # return response_info.read().decode('utf-8')  # 返回网页源码，如果需要更多的信息，改变调用的方法即可  
def saveDate(data_list, save_path):  # 保存数据  
    work_book = xlwt.Workbook(encoding="utf-8", style_compression=0)  # 创建xlwt表格对象  
    work_sheet = work_book.add_sheet("豆瓣电影top250", cell_overwrite_ok=True)  # 创建工作表，可覆盖重写  
    col = ("电影详情链接", "图片链接", "影片中文名", "影片外国名", "评分", "评价数", "概况", "相关信息")  
    # 列  
    for col_i in range(0, 8):  
        work_sheet.write(0, col_i, col[col_i])  
    for num in range(0, 250):  
        data = data_list[num]  # 第num个电影的信息  
        for col_j in range(0, 8):  
            work_sheet.write(num + 1, col_j, data[col_j])  
    work_book.save(save_path)  # 保存数据  
def saveDateDB(data_list, db_path):  
    # init_db(db_path)  
    conn = sqlite3.connect(db_path)  
    cur = conn.cursor()  
    for data in data_list:  
        for index in range(len(data)):  
            if index == 5 or index == 4:  
                continue  
            data[index] = '"' + str(data[index]) + '"'  
        sql = '''  
            insert into movie250(            info_link, pic_link, name, ename, score, rated, introduction, info)             values(%s)  
             ''' % ",".join(data)  # 此处是用%s先占位，然后将data里面8  
        # 列（一部电影）的所有信息分别用逗号隔开；将分割后的字符串放到占位的地方上去，此处简洁操作，值得学习。  
        cur.execute(sql)  
        conn.commit()  
    cur.close()  
    conn.close()  
def init_db(db_path):  
    sql = '''  
        create table movie250        (        id integer primary key autoincrement,        info_link text,        pic_link text,        name varchar,        ename varchar,        score numeric,        rated numeric,        introduction text,        info text        )    '''  # 创建数据表  
    conn = sqlite3.connect(db_path)  
    cursor = conn.cursor()  
    cursor.execute(sql)  
    conn.commit()  
    conn.close()  
if __name__ == '__main__':  
    main()
```
