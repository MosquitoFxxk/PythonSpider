# **PythonSpider：获取数据**
## 一、爬虫
### 1. 什么是爬虫？
爬虫是按照一定的**规则**，**自动搜集**网页上的数据。
### 2. 爬虫的本质？：
爬虫的本质是**模拟浏览器**打开网页，并获取保存网页当中的数据。
其中，浏览器：是指用来向服务器发送请求，并且将服务器响应发出的数据（html etc.）进行解析的应用程序。
### 3. 爬虫脚本的基本流程：
1. 获取网页数据
2. 解析网页数据
3. 保存数据
---
## 二、获取数据
###  包、模块：
Python里面包含许多第三方库，其中有用来获取网页数据的包或者模块，我们可以直接调用。
```python
from bs4 import BeautifulSoup
import re
import urllib.request, urllib.error，urllib.parse
import xlwt
import sqlite3
#说明：
'''
1.bs4是python用来解析网页可以从html文件或者xml文件中提取数据的包，我们主要用其中的BeautifulSoup模块
2.re模块：正则表达式，进行文字匹配
3.urllib，模拟浏览器，主要用其中的request对象，来指定url，获取网页数据
4.xlwt,进行excel操作，存储到excel当中
5.sqlite3，进行SQLite操作，即存储到数据库中
'''
```
---
###  **知识点目录**：
---
### **前言：**
获取数据是指利用urllib模块里面的request对象，模拟浏览器向服务器发送url，然后服务器响应返回数据，这个数据通常会是html，json等等文件。
然后一般情况下是浏览器会解析这个文件，从而我们看到网页的就是解析后的结果；不同的是，Python模拟浏览器得到服务器给出的数据以后，是直接得到的源码，就是说不会去解析文件。

模拟浏览器向服务器发出的信息是：
**Response Header**，**Request Header**
具体可在谷歌浏览器按键F12查询，会包含浏览器的版本信息，Cookie（反应我的ip地址，以前浏览过的网址，从哪访问来等等信息。若要爬去登录以后才能查询的信息，必须要传递Cookie）等等。

## 目录：
1. [发出一个get请求](#发出一个get请求)
2. [发出一个post请求](#发出一个post请求)
3. [超时处理](#超时处理)
4. [response对象](#response对象)
5. [修改header，防止418从我做起](#修改header，防止418从我做起)
6. [BeautifulSoup拓展以及文档遍历/搜索](#修改header，防止418从我做起)
7. [获取数据完整模板](#获取数据)
---
利用urllib模拟浏览器向服务器发出请求：
#### 1. 发出一个get请求
```python
import urllib.request
response = urllib.request.urlopen("http://www.baidu.com")
print(response) # 返回的是一个Html文件的对象，该对象里面存储了网页的数据信息
print(response.read()) # read()方法可以读取源码，但是里面会很乱，比如回车会以\n的形式出现
print(response.read().deconde('utf-8')) # 对获取到的网页源码进行utf-8的转码
```
#### 2. 发出一个post请求
```python
# 模拟用户真实登录时使用
import urllib.parse # 解析器
data = bytes(urllib.parse.urlencode({"name":"password"},encoding='utf-8') # urlencode按照一定的格式将键值对进行解析encoding='utf-8'将前面的解析结果按照某种格式，封装到二进制数据中。
#二进制数据 <-> utf-8 <-> urlencode <-> 字典
response = urllib.request.urlopen("http://www.baidu.com/post", data=data) # 注意网址的末尾会加上post; 其次需要给服务器发送一个表单，里面会包含一些信息，比如二进制信息
print(resopnse.read().decode('utf-8'))
```
[点击返回目录](#目录：)
#### 3. 超时处理
```python
try:  
   respose = urllib.request.urlopen("http://www.baidu.com",timeout=1)  # 响应时间超过1S，就会报错结束
   print(respose.read().decode('utf-8'))  # 对获取到的网页源码进行utf-8解码  
except urllib.error.URLError as time_out:  
    print("time out!")
```
#### 4. response对象
```python
# response对象,指的是服务器相应发给你的对象，里面包含了数据  
response = urllib.request.urlopen("http://www.baidu.com", timeout=1)
    print(response.status)
    # 200，是正常响应
    # 404，页面丢失
    # 418，你被发现啦，扒手！    
    print(response.getheaders())
    # 得到自己发送给服务器的信息，即响应头    
	print(response.getheader("Serve"))	# 得到header当中的serve数据
```
#### 5.修改header，防止418从我做起
```python
base_url = "https://www.douban.com"  
request_headers = {  
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"  
}  
data = bytes(urllib.parse.urlencode({}), encodings='utf-8')  
req = urllib.request.Request(url=base_url, data=data, headers=request_headers, method='POST')  
response = urllib.request.urlopen(req)  
print(respose.read(), encodings='utf-8')
```
代码解释：
将请求request重新封装为一个新的request对象，传递给urlopen，这样就能防止服务器识别出你是小扒手啦！
[点击返回目录](#目录：)
#### 6. BeautifulSoup拓展以及文档遍历/搜索

BeautifulSoup是bs4包内的一个模块，其主要用来将复杂的源码文件解析为复杂的树形结构文件，其每个节点都是一个python对象，主要分为四类：
tag：标签；NavigableString：标签内的内容；BeautifulSoup：对象本身；Comment：注释（会将注释符合去除，得到内容可能会与NavigableString一样）
```python
from bs4 import BeautifulSoup
file = open("./baidu.html","rb")
html = file.read().decode('utf-8')
bs = Beautiful(html, "html.parser") # 用html解析器去解析html文档，然后返回对象

print(bs) # 输出的是html文档，但是没有嵌套，二十树形结构，即BeautifulSoup
print(bs.a) # 输出的是html文档里的第一个a标签，即tag
print(bs.a.string) # 输出的是html文档里的第一个a标签的内容，即navegableString
print(bs.a) # 若文档里面此处为注释，比如<!--新闻-->，但此处输入为新闻，忽略了注释，即Comment
```
**文档遍历**
```python
# 文档遍历
print(bs.head.content)
print(bs.head.content[1])
# 此处是将head标签内的内容放到了列表当中
```
**文档搜索**
* **利用find_all()函数，与正则表达式搭配使用**
* **利用CSS选择器，即select()函数**
```python
# 文档搜索
import re
# 1.find_all()，返回的是列表
# 字符串过滤：会查找与字符串完全匹配的内容
text_list = bs.find_all("a")
print(text_list) # 会输出所有的a标签，注意，指的是标签有也仅有a才能匹配，若是abc，则不能匹配

# 正则表示搜索：使用search()方法来匹配内容
text_list = bs.find_all(re.compile("a"))
print(text_list) # 会输出文档标签中，任何一个只要包含a的内容，包括a标签，head标签等等

# 2.kwargs参数
text_list = bs.find_all(class_ = True)
text_list = bs.find_all(id = "head")
print(text_list) # 输出的是含有class属性的内容和他的子内容；第二个是输出标签head的内容及其子内容

# 3.text参数
text_list = bs.find_all(text = ["hao123","地图"])
print(text_list) # 输出含有文本hao123或者地图的内容，只会输出NavegableString，即内容

# 4.limit
text_list = bs.find_all("a", limit=3)
print(text_list) # 只会输出3个a标签内容
```
CSS选择器
```python
# CSS选择器
t = bs.select("title") # 通过标签来查找
t = bs.select(".mnav") # 通过类名来查找
t = bs.select("#u1") # 通过id来查找
t = bs.select("a[class='bri']") # 通过属性来查找
t = bs.select("head > title") # 通过子标签来查找
```
[点击返回目录](#目录：)

---
### **获取数据**
---
前言：
我们定义一个函数 getData（ base_url ）得到所有我们想要的所有数据，但是这个数据可能会分布在不同的页面当中，又因为**获取数据是一页一页获取**，所以我们要定义一个函数askURL(url)来得到一页的源码（数据），然后在getDate中去重复调用，每次都需要将传递的url更新。
**获取网页数据通常是一页一页逐一获取。**

[点击返回目录](#目录：)

---
```python
def getData(base_url):  # 获取网页数据  
    data_list = []  
    # 以豆瓣网为例，10页的数据  
    for i in range(0, 10):  
        url = base_url + str(i * 25)  
        response_info = askURL(url)  
        print(response_info)  
    return data_list  
  
  
def askURL(url): # 获取一页的网页数据  
    request_headers = {  
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"  
    }  # 注意拼写一定要正确，比如User-Agent，一定要严格按照格式，比如如果出现空格，User - Agent，则也会被服务器识别出来 
    try:  
        request = urllib.request.Request(url, headers=request_headers)  # 重新封装  
        response_info = urllib.request.urlopen(request)  # 模拟浏览器发送请求  
    except urllib.error.URLError as urlError:  # 异常处理，有可能会出现浏览器内部的问题，404等等问题  
        if hasattr(urlError, "code"):  
            print(urlError.code)  
        if hasattr(urlError, "reason"):  
            print(urlError.reason)  
    return response_info.read().decode('utf-8')  # 返回网页源码，如果需要更多的信息，改变调用的方法即可
```

